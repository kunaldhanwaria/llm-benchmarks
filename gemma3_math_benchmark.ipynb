{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HJFvnIO5r5q"
      },
      "source": [
        "# Evaluating Google Gemma 3 1B with LM Evaluation Harness\n",
        "\n",
        "> **Note:**  \n",
        "To use the Gemma model from Hugging Face:\n",
        "- You must be logged into your Hugging Face account.\n",
        "- You need to [agree to Google’s usage terms](https://huggingface.co/google/gemma-3-1b-it) on the model page.\n",
        "- A **Hugging Face access token** with access to the model is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-24w2Pcvdii",
        "outputId": "ec5901be-7591-474e-ceb3-225158791879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun May 25 12:41:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check if a GPU is available\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qdNdzbLq5_p",
        "outputId": "0ab4a4c4-93fb-4c94-bafd-54c58c35366a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3AJqXzFUWsiB",
        "outputId": "c32c6528-67c5-4401-a76a-d6a82bed7bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required libraries:\n",
        "!pip install bitsandbytes accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "2ea91bdd00b34648b1c00ac4cdbe0581",
            "bf1d7f48504947e1baf6d01f657cd91d",
            "0a37b1433c7f4906a78c93f80147433a",
            "e7f36e6194694f57b73eb1916417d92c",
            "2ef28904e6cf4718a7da753eea048ac2",
            "aac069413ea744539d8aeeb5dd807546",
            "c7109d5d20af421ca2f33e9430f25dc6",
            "890ab8bab0fa459db0e5fa9f5dfcf57a",
            "8df4d5b2c683438dbf594f4fcc9b0cf8",
            "5e12122c97f24d6696096c8d5de3583b",
            "532d7ad376b744de838ca3ce4a896fe7",
            "18f224ed7fa246b5a8be9167ecd1ea3d",
            "d3567b1944d645848b4a329c74a30c20",
            "1774ec061ef54404b6970b1645f7ab2e",
            "975c0bf1c815407192b5e3c5d25cc1d9",
            "d16b1fc4d89e4dfab8c5e7359e603c6b",
            "9b34efaebce94c7baaee418c6f254b72",
            "1e356f0db63d491283f78cf5d4bd0300",
            "2b898e0d41894336aa1e91ce7b64395d",
            "8a7e423ea7be449ca25bec04c94df1e1",
            "59ed69d86dca42309708659334af1d17",
            "480c218bf2c7419388b44206294e81c8",
            "a277d97cb5074b21b32bf0c8fffadc3b",
            "ba6f4ddc053a4133b012e9eb0143d1ab",
            "3c84636bb649443db645ce9ba3edb943",
            "6300aa38ffb04b5f972a1d9e98a80978",
            "052400b4091b43919dd58aac11550995",
            "b1ca5a1c10ce44d7a364375a12de8593",
            "a5c13381777f4fcb97a3d230ce797722",
            "8d3a4387c2df4bcd9951c828686fdc0c",
            "ad004c463873416685ae4b1693ec204a",
            "e6cd7b6dd06644cc80fd9fc135f5a8d5",
            "9c178f5156e44dc390a6604b4b64a9fc",
            "f86243429bc3446b97b92cb6759b322f",
            "197f570676fa4e01867e264daa0b4082",
            "d2f41e8a1c60413c92ec36df3ba4b876",
            "6b5b5182c51843838775055f7004869f",
            "522d5aa8a7014e39aab606054a598d5e",
            "2deb2d5255804538923f3aec9832c2af",
            "d1f28f6c4647431fa74d1d34da59e668",
            "af9c54444dc64c13a616438f505408f4",
            "28bf0fb1784549dda80decb415749fab",
            "8ff1da6f58ea4d9196221280fe533153",
            "b4cb59c71f52432a9f257f476d5b60b2",
            "c83e63753283487b92d148b546bae3e3",
            "39b3a34660404af8aae91896492a627c",
            "afbfed9f630d4bc8922aa93f389774cd",
            "abf22ea635d74673819c2037486a4241",
            "563c33d69fa2413b8d37c94f647e925d",
            "0f09befe0d2f4f589d79cbe87e5a7725",
            "0684d5a235444468abc0311774022d82",
            "3e174d60b1034068a2602c3171ca90d6",
            "5d15df2bfa62491d925faa13c8dcb7fd",
            "8a1c388de94b420eb559c5829889406a",
            "efad8f9558324efabee70abdadd8f038",
            "e9209a5df6044a16b310a72b9587378c",
            "54cc3aa11ae24908a4c7d1894b3debad",
            "028468365b91469d8c2177bb6370a07f",
            "0e8df135b21847a8b0d9c4b6c5dcb572",
            "842b157bb93e48d5a7424851fab54a72",
            "b308eb48ad72485c8940187f8c288fa0",
            "981c0da37bff47258468d3d3d0dab1f5",
            "37dfc4ef672b47299707eef5fc2881f3",
            "b586720cbdf44685b0fe9b888f63cbf1",
            "38da38123eca4e5dbde71a16d1736962",
            "624b15e69c3942de8b6d069662a86f80",
            "6062580eab0647bb97f17d28329752b6",
            "eb511bbfc3ba4e5c95cfde14fdad9b20",
            "618139aa466e411892b8c933a1c8d0c9",
            "c3fca42c4b8649aa8e0e46870988e456",
            "117b4caa50d04f2db9a195d8a375f976",
            "df012b35667e46458d09b7079aa6f73d",
            "63a8e38273b940f78d84aaa4fbbc490f",
            "2e9ed519dd5842daaa0f42640f804518",
            "5d43a2a6d1164f4d9e26e23c85deab7a",
            "e71a028c31e04b2daee5bc4b37edabb0",
            "624c9a335f9c44029d92c5fdfcca8f4b",
            "6adbaf586dee48f8a929f865bc02bd06",
            "5c4a49da31f9406f960696756a3940b8",
            "356e9dee056c452a821fbffdfedda148",
            "f43001d50ef244ab8fefe78fa6f48adb",
            "69cb510965924132beb51ceeba644a5c",
            "86aaaeb1a2f14123aa3f52b0296fe8eb",
            "9cfb146fe7f54f0eb27e4a1039dc916d",
            "1a89d9770c9646cd9335dff39df0d38e",
            "f34cac62c8264fd5b2ff03a9d10f93d1",
            "584e0a7258a1433b9ba4f9e58f1c17fc",
            "3940d53abb6f42f796fabb21e339b96d"
          ]
        },
        "collapsed": true,
        "id": "D0xV4yttimDZ",
        "outputId": "2c052584-0908-4f41-f57a-d0cc48a63e2d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ea91bdd00b34648b1c00ac4cdbe0581",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18f224ed7fa246b5a8be9167ecd1ea3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a277d97cb5074b21b32bf0c8fffadc3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f86243429bc3446b97b92cb6759b322f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c83e63753283487b92d148b546bae3e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9209a5df6044a16b310a72b9587378c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6062580eab0647bb97f17d28329752b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6adbaf586dee48f8a929f865bc02bd06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# Configure 4-bit quantization due to limited GPU VRAM\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_ASQN6HxA-C",
        "outputId": "cc287780-8528-4b6b-e7c9-825141acb1cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemma3TextConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"architectures\": [\n",
            "    \"Gemma3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attn_logit_softcapping\": null,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"cache_implementation\": \"hybrid\",\n",
            "  \"eos_token_id\": [\n",
            "    1,\n",
            "    106\n",
            "  ],\n",
            "  \"final_logit_softcapping\": null,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 1152,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 6912,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"gemma3_text\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 26,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"fp4\",\n",
            "    \"bnb_4bit_use_double_quant\": false,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"query_pre_attn_scalar\": 256,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_local_base_freq\": 10000,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": 512,\n",
            "  \"sliding_window_pattern\": 6,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 262144\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwhyT8rDmkpC",
        "outputId": "4b30d1d8-b6dc-4054-b9dc-abd94b6b57f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'lm-evaluation-harness'...\n",
            "remote: Enumerating objects: 13179, done.\u001b[K\n",
            "remote: Counting objects: 100% (13179/13179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5124/5124), done.\u001b[K\n",
            "remote: Total 13179 (delta 8154), reused 12447 (delta 8025), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (13179/13179), 3.90 MiB | 10.15 MiB/s, done.\n",
            "Resolving deltas: 100% (8154/8154), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Clone and install the lm-evaluation-harness for LLM evaluation:\n",
        "!git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!pip install -e ./lm-evaluation-harness -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHyIknXoBh1H",
        "outputId": "6048bc37-5a40-457a-d410-61269a4306f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-25 12:44:12.935445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748177052.956561    2094 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748177052.962534    2094 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "usage: lm_eval [-h] [--model MODEL] [--tasks task1,task2]\n",
            "               [--model_args MODEL_ARGS] [--num_fewshot N]\n",
            "               [--batch_size auto|auto:N|N] [--max_batch_size N]\n",
            "               [--device DEVICE] [--output_path DIR|DIR/file.json]\n",
            "               [--limit N|0<N<1] [--samples /path/to/json] [--use_cache DIR]\n",
            "               [--cache_requests {true,refresh,delete}] [--check_integrity]\n",
            "               [--write_out] [--log_samples]\n",
            "               [--system_instruction SYSTEM_INSTRUCTION]\n",
            "               [--apply_chat_template [APPLY_CHAT_TEMPLATE]]\n",
            "               [--fewshot_as_multiturn] [--show_config] [--include_path DIR]\n",
            "               [--gen_kwargs GEN_KWARGS]\n",
            "               [--verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG]\n",
            "               [--wandb_args WANDB_ARGS]\n",
            "               [--wandb_config_args WANDB_CONFIG_ARGS]\n",
            "               [--hf_hub_log_args HF_HUB_LOG_ARGS] [--predict_only]\n",
            "               [--seed SEED] [--trust_remote_code] [--confirm_run_unsafe_code]\n",
            "               [--metadata METADATA]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL, -m MODEL\n",
            "                        Name of model e.g. `hf`\n",
            "  --tasks task1,task2, -t task1,task2\n",
            "                        Comma-separated list of task names or task groupings to evaluate on.\n",
            "                        To get full list of tasks, use one of the commands `lm-eval --tasks {{list_groups,list_subtasks,list_tags,list}}` to list out all available names for task groupings; only (sub)tasks; tags; or all of the above\n",
            "  --model_args MODEL_ARGS, -a MODEL_ARGS\n",
            "                        Comma separated string or JSON formatted arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32` or '{\"pretrained\":\"EleutherAI/pythia-160m\",\"dtype\":\"float32\"}'\n",
            "  --num_fewshot N, -f N\n",
            "                        Number of examples in few-shot context\n",
            "  --batch_size auto|auto:N|N, -b auto|auto:N|N\n",
            "                        Acceptable values are 'auto', 'auto:N' or N, where N is an integer. Default 1.\n",
            "  --max_batch_size N    Maximal batch size to try with --batch_size auto.\n",
            "  --device DEVICE       Device to use (e.g. cuda, cuda:0, cpu).\n",
            "  --output_path DIR|DIR/file.json, -o DIR|DIR/file.json\n",
            "                        Path where result metrics will be saved. Can be either a directory or a .json file. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\n",
            "  --limit N|0<N<1, -L N|0<N<1\n",
            "                        Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.\n",
            "  --samples /path/to/json, -E /path/to/json\n",
            "                        JSON string or path to JSON file containing doc indices of selected examples to test. Format: {\"task_name\":[indices],...}\n",
            "  --use_cache DIR, -c DIR\n",
            "                        A path to a sqlite db file for caching model responses. `None` if not caching.\n",
            "  --cache_requests {true,refresh,delete}\n",
            "                        Speed up evaluation by caching the building of dataset requests. `None` if not caching.\n",
            "  --check_integrity     Whether to run the relevant part of the test suite for the tasks.\n",
            "  --write_out, -w       Prints the prompt for the first few documents.\n",
            "  --log_samples, -s     If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis. Use with --output_path.\n",
            "  --system_instruction SYSTEM_INSTRUCTION\n",
            "                        System instruction to be used in the prompt\n",
            "  --apply_chat_template [APPLY_CHAT_TEMPLATE]\n",
            "                        If True, apply chat template to the prompt. Providing `--apply_chat_template` without an argument will apply the default chat template to the prompt. To apply a specific template from the available list of templates, provide the template name as an argument. E.g. `--apply_chat_template template_name`\n",
            "  --fewshot_as_multiturn\n",
            "                        If True, uses the fewshot as a multi-turn conversation\n",
            "  --show_config         If True, shows the the full config of all tasks at the end of the evaluation.\n",
            "  --include_path DIR    Additional path to include if there are external tasks to include.\n",
            "  --gen_kwargs GEN_KWARGS\n",
            "                        Either comma delimited string or JSON formatted arguments for model generation on greedy_until tasks, e.g. '{\"temperature\":0.7,\"until\":[\"hello\"]}' or temperature=0,top_p=0.1.\n",
            "  --verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG, -v CRITICAL|ERROR|WARNING|INFO|DEBUG\n",
            "                        (Deprecated) Controls logging verbosity level. Use the `LOGLEVEL` environment variable instead. Set to DEBUG for detailed output when testing or adding new task configurations.\n",
            "  --wandb_args WANDB_ARGS\n",
            "                        Comma separated string arguments passed to wandb.init, e.g. `project=lm-eval,job_type=eval\n",
            "  --wandb_config_args WANDB_CONFIG_ARGS\n",
            "                        Comma separated string arguments passed to wandb.config.update. Use this to trace parameters that aren't already traced by default. eg. `lr=0.01,repeats=3\n",
            "  --hf_hub_log_args HF_HUB_LOG_ARGS\n",
            "                        Comma separated string arguments passed to Hugging Face Hub's log function, e.g. `hub_results_org=EleutherAI,hub_repo_name=lm-eval-results`\n",
            "  --predict_only, -x    Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\n",
            "  --seed SEED           Set seed for python's random, numpy, torch, and fewshot sampling.\n",
            "                        Accepts a comma-separated list of 4 values for python's random, numpy, torch, and fewshot sampling seeds, respectively, or a single integer to set the same seed for all four.\n",
            "                        The values are either an integer or 'None' to not set the seed. Default is `0,1234,1234,1234` (for backward compatibility).\n",
            "                        E.g. `--seed 0,None,8,52` sets `random.seed(0)`, `torch.manual_seed(8)`, and fewshot sampling seed to 52. Here numpy's seed is not set since the second value is `None`.\n",
            "                        E.g, `--seed 42` sets all four seeds to 42.\n",
            "  --trust_remote_code   Sets trust_remote_code to True to execute code to create HF Datasets from the Hub\n",
            "  --confirm_run_unsafe_code\n",
            "                        Confirm that you understand the risks of running unsafe code for tasks that require it\n",
            "  --metadata METADATA   JSON string metadata to pass to task configs, for example '{\"max_seq_lengths\":[4096,8192]}'. Will be merged with model_args. Can also be set in task config.\n"
          ]
        }
      ],
      "source": [
        "!lm_eval -h # Lists all available commands for running benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTIdGQkoqWDu",
        "outputId": "1a079d34-e263-4005-b3a6-418f06df64aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ],
      "source": [
        "# Automatically configures device setup (CPU/GPU) and other settings\n",
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdcD3s1-omfo",
        "outputId": "2d168e5a-2f55-4a9d-9c09-4e87c1a5e503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-25 12:44:45.943641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748177085.979978    2321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748177085.990125    2321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING:__main__: --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "INFO:__main__:Selected Tasks: ['gsm8k']\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Initializing hf model, with arguments: {'pretrained': 'google/gemma-3-1b-it', 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16'}\n",
            "INFO:lm_eval.models.huggingface:Using device 'cuda:0'\n",
            "INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "INFO:lm_eval.models.huggingface:Model type is 'gemma3_text', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\n",
            "README.md: 100% 7.94k/7.94k [00:00<00:00, 28.8MB/s]\n",
            "train-00000-of-00001.parquet: 100% 2.31M/2.31M [00:00<00:00, 132MB/s]\n",
            "test-00000-of-00001.parquet: 100% 419k/419k [00:00<00:00, 360MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 85953.19 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 124707.79 examples/s]\n",
            "INFO:lm_eval.evaluator:gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of gsm8k from 5 to 0\n",
            "INFO:lm_eval.api.task:Building contexts for gsm8k on rank 0...\n",
            "100% 10/10 [00:00<00:00, 1506.47it/s]\n",
            "INFO:lm_eval.evaluator:Running generate_until requests\n",
            "Running generate_until requests:   0% 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 10/10 [03:06<00:00, 18.68s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "INFO:lm_eval.loggers.evaluation_tracker:Saving results aggregated\n",
            "hf (pretrained=google/gemma-3-1b-it,load_in_4bit=True,bnb_4bit_compute_dtype=bfloat16), gen_kwargs: (None), limit: 10.0, num_fewshot: 0, batch_size: 1\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
            "|gsm8k|      3|flexible-extract|     0|exact_match|↑  |  0.2|±  |0.1333|\n",
            "|     |       |strict-match    |     0|exact_match|↑  |  0.0|±  |0.0000|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation using lm-evaluation-harness with Accelerate\n",
        "# - Model: google/gemma-3-1b-it loaded in 4-bit precision using bitsandbytes\n",
        "# - Task: GSM8k (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems.\n",
        "# - Few-shot setting: 0 (zero-shot evaluation - number of examples included in the prompt to guide the model)\n",
        "# - Batch size: 1 (evaluate one sample at a time)\n",
        "# - Limit: Run only 10 samples for quick testing\n",
        "\n",
        "!accelerate launch -m lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args \"pretrained=google/gemma-3-1b-it,load_in_4bit=True,bnb_4bit_compute_dtype=bfloat16\" \\\n",
        "  --tasks gsm8k \\\n",
        "  --num_fewshot 0 \\\n",
        "  --device cuda:0 \\\n",
        "  --batch_size 1 \\\n",
        "  --limit 10 \\\n",
        "  --output_path ./output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0mzi-a46HRB"
      },
      "source": [
        "# To perform inference with the Gemma 3 1B model in 4-bit precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoAbJ95dWtWK",
        "outputId": "017bdf6c-9045-4074-a011-0ab79fe87333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model Response ---\n",
            "Explain the concept of quantization of LLMs in simple terms.\n",
            "\n",
            "(A) Randomly flipping pixels to represent the LLM\n",
            "(B) The LLM is a set of interconnected nodes and wires, running a complex calculation\n",
            "(C) The LLM is just a very large language model that can be used for other tasks\n",
            "(D) The LLM is a set of processes, each of which is simplified and repeated\n",
            "*\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "The concept of quantization in the context of LLMs refers to the process of representing the large, complex mathematical operations performed by the model (weights and activations) with smaller numbers, specifically bits. This is done to reduce the memory footprint and computational requirements for running the model on devices with limited resources, such as consumer-grade GPUs.\n",
            "\n",
            "Let'\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Explain the concept of quantization of LLMs in simple terms.\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150\n",
        ")\n",
        "\n",
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Model Response ---\")\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewXtR32u2tku"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
